{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"finans.csv\")\n",
    "\n",
    "idx = 0\n",
    "\n",
    "chunk = df.iloc[idx:idx+1]\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prefix = \"\"\"\n",
    "You are an expert data analyst. I will provide you with one row of data from a dataframe, containing several metrics. For each metric in the row, perform a thorough analysis and provide the following details for each:\n",
    "\n",
    "1. **Metric Name**: The name of the metric.\n",
    "2. **Trend**: Describe the overall trend of the metric (e.g., increasing, decreasing, stable, fluctuating).\n",
    "3. **Change Rate**: Provide the rate of change over time for the metric (e.g., percentage increase or decrease, or \"N/A\" if not applicable).\n",
    "4. **Current Status**: Summarize the current state of the metric (e.g., high, low, average).\n",
    "5. **Anomalies**: Indicate whether there are any anomalies in the metric and provide a description if anomalies are detected.\n",
    "6. **Summary**: Summarize the key findings for the metric in a brief statement.\n",
    "\n",
    "Ensure that each metric is analyzed thoroughly, and respond with raw, unfiltered output for all metrics in this format:\n",
    "\n",
    "- Metric Name: [Name]\n",
    "- Trend: [Trend]\n",
    "- Change Rate: [Rate]\n",
    "- Current Status: [Status]\n",
    "- Anomalies: [True/False, Description if True]\n",
    "- Summary: [Summary]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_format_prompt_string = \"\"\"\n",
    "Reorganize the given text according to the output format {format_instructions}.\n",
    "\n",
    "The given text is:\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_prompt = PromptTemplate(\n",
    "    input_variables=[\"format_instructions\", \"response\"],\n",
    "    template=output_format_prompt_string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAnalysis(BaseModel):\n",
    "    metric_name: str = Field(description=\"The name of the metric being analyzed. Example: 'Revenue', 'User Growth Rate', 'Profit Margin'\")\n",
    "    trend: str = Field(description=\"The overall trend of the metric. Example: 'Increasing', 'Decreasing', 'Stable', 'Fluctuating'\")\n",
    "    change_rate: str = Field(description=\"The rate of change over time for the metric. Example: '10% increase', '5% decrease', 'N/A'\")\n",
    "    current_status: str = Field(description=\"The current status of the metric. Example: 'High', 'Low', 'Average'\")\n",
    "    anomalies: bool = Field(description=\"Whether any anomalies are detected in the metric. Example: True, False\")\n",
    "    summary: str = Field(description=\"A brief summary of the key findings for the metric.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MetricAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Analyze the following metrics:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "llm_gemma = ChatOllama(model=\"gemma2:9b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â OPENAI + GEMMA\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm_gemma,\n",
    "    chunk,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    allow_dangerous_code=True,\n",
    "    max_iterations=3,\n",
    "    prefix=agent_prefix,\n",
    ")\n",
    "\n",
    "try:\n",
    "    first_response = agent.invoke(query)[\"output\"]\n",
    "except Exception as e:\n",
    "    first_response = str(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = output_prompt | llm_gemma | parser \n",
    "\n",
    "second_response = chain.invoke({\"format_instructions\": parser.get_format_instructions(), \"response\": first_response})\n",
    "\n",
    "print(second_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMINI + GEMMA\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm_gemini,\n",
    "    chunk,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    allow_dangerous_code=True,\n",
    "    max_iterations=3,\n",
    "    prefix=agent_prefix,\n",
    ")\n",
    "\n",
    "try:\n",
    "    first_response = agent.invoke(query)[\"output\"]\n",
    "except Exception as e:\n",
    "    first_response = str(e) \n",
    "\n",
    "chain = output_prompt | llm_gemma | parser \n",
    "\n",
    "second_response = chain.invoke({\"format_instructions\": parser.get_format_instructions(), \"response\": first_response})\n",
    "\n",
    "print(second_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMMA + GEMMA\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm_gemma,\n",
    "    chunk,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    allow_dangerous_code=True,\n",
    "    max_iterations=3,\n",
    "    prefix=agent_prefix,\n",
    ")\n",
    "\n",
    "try:\n",
    "    first_response = agent.invoke(query)[\"output\"]\n",
    "except Exception as e:\n",
    "    first_response = str(e) \n",
    "\n",
    "chain = output_prompt | llm_gemma | parser \n",
    "\n",
    "second_response = chain.invoke({\"format_instructions\": parser.get_format_instructions(), \"response\": first_response})\n",
    "\n",
    "print(second_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
