{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"finans.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "#llm = ChatOllama(model=\"gemma2:9b\", temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(\n",
    "                llm,\n",
    "                df,\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "                # agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                allow_dangerous_code=True,\n",
    "                max_iterations=3,\n",
    "                number_of_head_rows=df.shape[0],\n",
    "        )\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "For every response, follow this strict format:\n",
    "\n",
    "1. Thought: Provide your thought process.\n",
    "2. Action: State the action you will take (e.g., analyze_column).\n",
    "3. Action Input: Provide the input for the action (e.g., the column name).\n",
    "4. Final Answer: Provide your conclusion or analysis.\n",
    "\n",
    "Can you describe the given dataset? What is the dataset about? What are the columns and what do they represent?\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "try:\n",
    "    response = agent.invoke(query)[\"output\"]\n",
    "except Exception as e:\n",
    "    response = str(e) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"agent_output\": \"\"\"An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Parsing LLM output produced both a final answer and a parse-able action:: 1. **Thought:** The dataset appears to track financial metrics over time. The first column, \"Metrics\", likely lists the specific financial measures being tracked. The remaining columns represent different years from 2014 to 2023.\n",
    "\n",
    "\n",
    "2. **Action:** python_repl_ast\n",
    "3. **Action Input:** print(df.head())\n",
    "4. **Observation:**  \n",
    "\n",
    "```\n",
    "            Metrics       2014       2015       2016       2017       2018  \\\n",
    "0      Net Revenue   71537.14  122985.28  191166.93  210916.53   28892.26   \n",
    "1  Operating Costs   13802.18   90355.33  132564.46   65384.14  278429.73   \n",
    "2     Gross Profit   37777.14  122216.88   46822.39  194469.15  148443.40   \n",
    "3            Taxes  179213.45  219638.18  131592.86  209942.17  185695.02   \n",
    "4       Net Income  297151.16   83893.72   62105.06   47566.50   61001.34   \n",
    "\n",
    "        2019       2020       2021       2022       2023  \n",
    "0  158131.90   55412.42   44390.00  197604.56  125449.27  \n",
    "1   66884.82   25662.97  165766.93  102527.72   49780.61  \n",
    "2  191415.56  295809.37  134085.72  227645.00   18907.24  \n",
    "3  190911.59   78370.23  230701.83  120794.54  199445.19  \n",
    "4   45859.36  182117.34  202499.03  184534.90  286222.75 \n",
    "```\n",
    "\n",
    "Thought: The dataset contains financial metrics for different years. Each row represents a specific metric (e.g., Net Revenue, Operating Costs), and each column represents a year from 2014 to 2023.\n",
    "\n",
    "\n",
    "\n",
    "Final Answer: The dataset tracks various financial metrics over time. It includes\"\"\",\n",
    "\n",
    "        \"summary\": \"\"\"The financial table provided tracks different financial metrics over a span of ten years, from 2014 to 2023. The first column, labeled \"Metrics,\" lists specific financial measures such as Net Revenue, Operating Costs, Gross Profit, Taxes, and Net Income. Each row corresponds to a particular metric, while each subsequent column represents a different year. The table showcases the values of these metrics for each year, offering a comprehensive overview of the company's financial performance over the specified period.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"agent_output\": \"\"\"An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Parsing LLM output produced both a final answer and a parse-able action:: 1. **Thought:** The provided data appears to be a table with numerical values.  We need to examine each row and column to understand its meaning. \n",
    "\n",
    "2. **Action:** analyze_columns\n",
    "\n",
    "3. **Action Input:** All columns\n",
    "\n",
    "4. **Final Answer:** The dataset seems to track financial or market data over a period of time (likely daily). Here's a breakdown:\n",
    "\n",
    "* **Column 1 (Number):**  Likely a unique identifier for each day (e.g., record number).\n",
    "* **Column 2 (Date):** Represents the date for each data point.\n",
    "* **Columns 3-8:** These columns likely contain numerical values representing different financial indicators. Without further context, it's difficult to say precisely what they represent.  Possible interpretations include:\n",
    "    * Opening price, closing price, high/low prices of a security or market index.\n",
    "    * Trading volume, market capitalization, or other financial metrics.\n",
    "\n",
    "To get a more accurate understanding, we would need additional information about the source of the data and the specific definitions of each column. \n",
    "\"\"\",\n",
    "\n",
    "        \"summary\": \"\"\"The financial table provided appears to track financial or market data over a period of time, likely on a daily basis. \n",
    "\n",
    "The columns in the table are as follows:\n",
    "1. Column 1 (Number): Likely a unique identifier for each day, such as a record number.\n",
    "2. Column 2 (Date): Represents the date for each data point.\n",
    "3. Columns 3-8: These columns likely contain numerical values representing different financial indicators. Without further context, it is challenging to precisely determine what they represent. Possible interpretations include opening price, closing price, high/low prices of a security or market index, trading volume, market capitalization, or other financial metrics.\n",
    "\n",
    "To gain a more accurate understanding of the data, additional information about the source of the data and the specific definitions of each column would be required.\"\"\",\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Agent Output: {agent_output}\n",
    "Summary: {summary}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"agent_output\", \"summary\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"\n",
    "You will be given an output from an AI agent that is responsible to analyze financial tables.\n",
    "\n",
    "Can you provide a summary of the given text? Do not include any insights or trends or any other analysis. Just describe the columns and what they represent, and the rows and what they represent. And in general what is this table?\n",
    "\n",
    "Just focus on what the financial table is about, ignore the actions, observations, errors and other logs that the agent produced.\n",
    "\n",
    "Tell the findings and descriptions like you are reporting to finance banking CEO in a good storytelling way. Use the original words from the agent output as much as possible.\n",
    "\n",
    "Examples: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "{format_instructions}\n",
    "Agent Output: {agent_output}\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"agent_output\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel):\n",
    "    text: str = Field(description=\"Summary of the given text.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = chain.invoke({\"format_instructions\": parser.get_format_instructions(), \"agent_output\": response}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE SCORE\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "scores = scorer.score(second_response, response)\n",
    "\n",
    "scores\n",
    "\n",
    "# DRAW GRAPH\n",
    "# AUTO LLM EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic textual similarity\n",
    "semantic_model = SentenceTransformer(\"mrm8488/gte-large-ft-webinstruct\",trust_remote_code=True)\n",
    "\n",
    "embeddings1 = semantic_model.encode(response)\n",
    "embeddings2 = semantic_model.encode(second_response)\n",
    "\n",
    "similarities = semantic_model.similarity(embeddings1, embeddings2)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "output_1 = model.encode(response, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "output_2 = model.encode(second_response, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "# Initialize the model\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "\n",
    "\n",
    "# Encode the two texts\n",
    "output_1 = model.encode(response, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "output_2 = model.encode(second_response, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
    "\n",
    "# Check the lexical weights of the first text\n",
    "lexical_weights_text1 = model.convert_id_to_token(output_1['lexical_weights'])\n",
    "lexical_weights_text2 = model.convert_id_to_token(output_2['lexical_weights'])\n",
    "\n",
    "# Print token and their lexical weights\n",
    "print(f\"Lexical weights for text1: {lexical_weights_text1}\")\n",
    "print(f\"Lexical weights for text2: {lexical_weights_text2}\")\n",
    "# Filter out non-string keys from lexical weights\n",
    "def filter_lexical_weights(lexical_weights):\n",
    "    return {k: v for k, v in lexical_weights.items() if isinstance(k, str)}\n",
    "\n",
    "# Get the filtered lexical weights\n",
    "filtered_lexical_weights_1 = filter_lexical_weights(output_1['lexical_weights'])\n",
    "filtered_lexical_weights_2 = filter_lexical_weights(output_2['lexical_weights'])\n",
    "\n",
    "# Compute lexical matching score between the two filtered texts\n",
    "lexical_scores = model.compute_lexical_matching_score(filtered_lexical_weights_1, filtered_lexical_weights_2)\n",
    "\n",
    "# Print the lexical matching score\n",
    "print(f\"Lexical matching score: {lexical_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1['lexical_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the scores via lexical mathcing\n",
    "lexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\n",
    "print(lexical_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer([response, second_response], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = F.cosine_similarity(sentence_embeddings[0], sentence_embeddings[1], dim=0)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "\n",
    "print(\"\\nCosine similarity between the two sentences:\")\n",
    "print(cosine_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASKING TO OTHER LLMs\n",
    "other_llm = ChatOllama(model=\"llama3.1:latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string = \"\"\"\n",
    "You will be given two texts. Your task is to compare the two texts and detect if they express the same information or not.\n",
    "\n",
    "TEXT 1: {text1}\n",
    "TEXT 2: {text2}\n",
    "\n",
    "Please answer with \"Yes\" if the two texts express the same information, or \"No\" otherwise.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text1\", \"text2\"],\n",
    "    template=prompt_string\n",
    ")\n",
    "\n",
    "class OtherCheck(BaseModel):\n",
    "    check: str = Field(description=\"Whether the two texts express the same information or not. (Yes or No)\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=OtherCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_chain = prompt | other_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_response = other_chain.invoke({\"text1\": response, \"text2\": second_response, \"format_instructions\": parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(other_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
